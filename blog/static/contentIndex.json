{"First-Blog":{"title":"My Hello World","links":[],"tags":[],"content":"Hey it works! Nice\nDoes it ever start without a Hello World?"},"Learnings/Amazon-Layoffs":{"title":"The 2023 AWS Layoffs and Personal Insights","links":[],"tags":["Layoffs","Amazon","AWS"],"content":"In an unprecedented turn of events, Amazon Web Services (AWS) recently faced a round of layoffs. This is a first in the history of AWS, marking a break from the past when even the tumultuous times of the 2008 financial crisis saw no layoffs at Amazon. I would like to share my insights and experiences related to this significant event.\nThe Announcement\nMy journey as a full-time software engineer at AWS was barely a month old, fresh from my university graduation, when the startling news of layoffs came through. It wasn’t an entirely new phenomenon at Amazon, as several divisions had previously experienced job cuts, mostly because they were not profitable. Hence, a reduction in expenditure was a strategic move towards achieving financial stability. But the AWS scenario was a different ball game. It was a profit hub, contributing approximately 50% of Amazon’s overall earnings. Consequently, layoffs within AWS seemed unwarranted, at least in my initial perspective. But this notion was dispelled when I read from TechCrunch: “Amazon confirms another round of layoffs, impacting 9,000 people in AWS, Twitch and other units.” (March 20, 2023).\nThis announcement sparked an influx of doubts and questions concerning the criteria for the layoffs. Despite reassurances that our team’s indispensable product would shield us from the layoffs, my anxiety levels were still heightened, primarily because I was the newest team member. The prospect of being the first exit due to my recent entry was quite nerve-wracking. But I was not the only one feeling the pressure. A surge in the internal Slack channel’s ‘layoff-discussion,’ with its membership skyrocketing to 60,000. The channel was abuzz with discussions about the impending layoffs, their timing, location, and the general emotional roller-coaster ride that ensued.\nThe Layoff\nThe dreaded day, Wednesday, April 26th, 2023, dawned with approximately 8,000 AWS employees receiving their termination emails at precisely 7:00 AM. Barely three minutes later, access to all company resources, including Slack, development environments, and emails, was revoked for those affected. My phone buzzed at 7:05 AM with a heartbreaking message from a close friend and talented engineer who was among the unfortunate ones. My organization had 27 people across 460 leave. 1 was in my team and 2 from our sister team.\nWhile I was about four levels below the decision-making executives, I heard the layoffs were generally orchestrated at the director level. However, the experiences varied across different departments. My analysis of the AWS Cryptography division, where I work, revealed a trend. The layoffs targeted underperformers, employees in non-aligned locations, non-software engineer roles, and SDE 1s (the entry level software engineer role). Non-aligned locations referred to remote employees or those working away from their teams or managers. Two of the best engineers I knew were laid off due to location mismatches.\nThe possibility of layoffs in our team was quite bewildering, given our product’s vital role as a Tier 0 product crucial to all internal services. Surprisingly, even teams associated with the most profitable AWS services like EC2, Lambda, and S3 experienced layoffs. It appeared as if stringent criteria were set, and anyone failing to meet these criteria was shown the door."},"Learnings/My-Journey;-High-School-to-Amazon":{"title":"My Journey: High School to Amazon","links":[],"tags":["Amazon","Learnings"],"content":""},"ML/I-felt-the-World-Change;-The-Release-of-ChatGPT":{"title":"I felt the World Change: The Release of ChatGPT","links":[],"tags":["GPT"],"content":"On December 1st, 2022, I watch the world change. As I sat in front of my computer, I couldn’t believe my eyes. As I continued to use chatGPT, I was struck by how intuitive and user-friendly it was. I didn’t need to have any prior knowledge or experience with language models to be able to use it effectively. Its user interface was simple and straightforward, making it easy for anyone to start using chatGPT right away.\nOne of the most impressive aspects of chatGPT was its ability to understand and respond to natural language. I could ask it questions in the same way that I would ask a friend, and it would provide detailed and accurate answers. This made it incredibly easy to have a conversation with chatGPT, and I found myself using it more and more as a go-to source for information.\nI was also impressed by the speed at which chatGPT was able to generate text. I could ask it to generate a paragraph or even an entire essay in a matter of seconds, and the resulting text was always of high quality. This made it an incredibly useful tool for anyone who needs to produce written content quickly and efficiently.\nOverall, my experience with chatGPT was nothing short of amazing. It was an incredible tool that has the potential to revolutionize the way we access and use information. I can’t wait to see what the future holds for chatGPT and the world of artificial intelligence.\nIn fact ChatGPT wrote this entire blog post.\nUpdate: Jan. 05, 2023\nHi, the human here.\nYes that was a little ingenuine on my part for leading you on like that, I am sorry about that. But it does a great job right?\nBut I really want to talk about this incredible new and exciting tool. For a long time I always had wondered what it would be like to experience a new and life changing product. The biggest game changing product that I remember in my life was the iPhone. I was a child then and I vaguely remember it. Even then I could not conceptualize what this product really was. And for a long time I wished that I could experience this change again and to see how life would change forever because of it.\nHere is a quick list of how long it took modern tech websites to reach 1 million active daily users:\n\nAirbnb: 30 months\nTwitter: 24 months\nFacebook: 10 months\nDropbox: 7 months\nSpotify: 5 months\nInstagram: 2.5 months\nChatGPT: 5 days\n\nThe craziest part is all of my interactions about this product were ONLY communicated by talking to other people. No social media campaigns. No podcast shoutouts. No finding on twitter. Just from friends and seeing people use it. 5 days"},"ML/Learning-ML/Gaussian-(Normal)-Distribution":{"title":"Gaussian (Normal) Distribution","links":[],"tags":[],"content":"Mean Vector \\mu\n\nthe mean vector represents the central point or the expected value of the distribution,\nyou can have multiple dimensions for a mean vector which would be like \\mu=[\\mu_1,\\mu_2]\n\nCovariance Matrix \\sum\n\nThe covariance matrix Σ in a multivariate Gaussian distribution captures how two variables change together (covary). It provides information on the extent to which two variables are linearly related to each other and the scale of these variables.\nIn a standard Gaussian distribution, the covariance matrix includes variances along the diagonal and covariances off the diagonal. Variance measures how much a set of values is spread out around the mean, and covariance measures how much two values vary from their mean together.\n\nDiagonal Covariance Matrix\n\nA diagonal Gaussian distribution is a specific case where the variables are assumed to be independent of each other, which means the covariances are zero.\n\nExample:\nHere are the examples of a covariance matrix and a diagonal covariance matrix for a 3-dimensional Gaussian distribution:\nGeneral Covariance Matrix\n[[1.0, 0.5, 0.3],\n [0.5, 1.0, 0.6],\n [0.3, 0.6, 1.0]]\n\nIn this general covariance matrix, the diagonal elements (1.0, 1.0, 1.0) represent the variances of each individual variable, and the off-diagonal elements (e.g., 0.5, 0.3, 0.6) represent the covariances between each pair of variables. These covariances indicate that there is some degree of linear relationship between the variables.\nDiagonal Covariance Matrix\n[[1.0, 0.0, 0.0],\n [0.0, 1.0, 0.0],\n [0.0, 0.0, 1.0]]\n\nIn the diagonal covariance matrix, all off-diagonal elements are zero, indicating that there is no linear relationship between the different variables—they are independent. The diagonal still contains the variances for each variable, identical to the general covariance matrix. This simplification often makes computations easier and is a common assumption when the independence of variables is a reasonable approximation."},"ML/Learning-ML/Learning-ML-Resources":{"title":"Learning ML Resources","links":[],"tags":[],"content":"OpenAI Spinning Up in Deep RL\nJacob Hilton Deep Learning Curriculum\n\nthis one is quite open ended and has lots of experiments to do it yourself.\nit does point to some good resources for the prerequisites needed to understand the material.\n\nStanford CS224N: Natural Language Processing with Deep Learning\n\nWinter 2021/3 Youtube Playlist Lectures\nTotal Watch Time 25 Hours 30 mins: 19 Lectures each at 1 hr 20 mins\n\nBlogs\nLil’Log\n\nOpen AI researcher, some very interesting blogs\n\nHow to Train Really Large Models on Many GPUs\n\n\n\nHow I got into OpenAI Rai\n\nTake aways\n\nRecommended process to repeat\n\nMake a list of relevant things in ML that I don’t understand\nFind resources to understand it\n\nCourseras\nreadings etc\n\n\nLearn all I can from those resources, collect stuff I dont yet understand, and find references for those things you dont understand\n\n\nRead up on recent papers\nRead Spinngin Up on Deep RL from Open AI\n\nsimilar but might be better Deep Learning Curriculum from Jacob Hilton\n\n\n\n\n\nNLP Course For You\n\nFamiliar, I think I read most of it before.\n\nTransformers From Scratch\n\nTop down approach to Transformers with some good follow up resources at the bottom\n\nScaling Kubernetes to 7,500 Nodes"},"ML/Learning-ML/LoRa-Low-Rank-Adaptation-of-Large-Language-Models":{"title":"LoRa: Low Rank Adaptation of Large Language Models Paper Review","links":[],"tags":["LLMs","Machine-Learning"],"content":"LoRa: Low Rank Adaptation of Large Language Models Paper Review\nHere is the paper I will be writing about today. LoRa: Low-Rank Adaptation of Large Language Models Paper\n1. Overview/Background:\nIntroduction to Large Language Models and Training:\nLarge Language Models (LLMs), consisting of massive matrices filled with numbers running into hundreds of gigabytes, are sophisticated tools in the AI sphere. Training these models typically involves two steps - pre-training and fine-tuning.\nPre-training: This phase involves training the model on a vast corpus of text, enabling it to acquire general language capabilities. During this stage, the model primarily serves as a ‘next-word predictor.’ The learning process during this phase is computationally expensive, thus incurring most of the operation cost. The model assimilates a wealth of knowledge but lacks the capability to effectively output it. For example, if the model is posed with a question, it might respond with related questions rather than providing a succinct answer.\nFine-tuning: This phase ensues post pre-training, where the model is trained on a smaller, task-specific dataset to acquire desired responses. For example, if the model is meant to function like chatGPT, it would be trained on a dataset of Questions and Answers similar to chatGPT. The training in this phase features smaller ‘steps,’ ensuring that each iteration only slightly alters the model’s weights.\n2. The Problem:\nThe significant issue with full fine-tuning is that every training iteration changes all the model parameters. This implies that the model needs to relearn and adjust all parameters for each iteration, which for larger models like GPT3 (with 175 Billion parameters) is a colossal computational task. Furthermore, fine-tuning the model for each task multiplies the storage needs. These challenges make the deployment of multiple instances of fine-tuned models practically infeasible.\n3. Solution\nLoRa addresses this challenge by proposing a low-rank adaptation of the model.\nGeneralized Formula\n\nOutput vector: Weights * Input vector\nWeights of pretrained LLM: W_0\nFormula for calculating weight matrices when finetuning: W_1 = W_0 + \\Delta W\nFormula for using finetuned weight matrices: h = W_1 * x = W_0 * x + \\Delta W * x\n\nWhat LoRA Proposes\nLoRA proposes that we do rank decomposition on the change W into two smaller matrices A and B. A and B will try to capture the change in W into smaller matrices that are easier to train and deploy. The formula for rank decomposition is as follows:\n\\Delta W = A * B\nwhere A and B are the rank-decomposition matrices.\nThen substite in the rank-decomposition matrices into the finetuning equation: h = W_0 * x + A * B * x\nDuring the finetuning process, the pre-trained weights are frozen, with modifications permitted only in A and B. Setting a low rank can substantially reduce the parameters to tune.\nBenefits of LoRA\n\nReduced training time: LoRA can significantly reduce the training time of LLMs, since only a small number of parameters need to be updated.\nReduced memory requirements: LoRA can also reduce the memory requirements of LLMs, since the number of parameters is much smaller.\nLess prone to catastrophic forgetting: LoRA is less prone to catastrophic forgetting, since the pre-trained weights are not updated.\nEasier to deploy: LoRA weights are easier to deploy than full fine-tuned models, since they are much smaller.\n\nLimitations of LoRA\n\nMay not be as effective as full fine-tuning: LoRA may not be as effective as full fine-tuning for some tasks.\nMay require more hyperparameter tuning: LoRA may require more hyperparameter tuning than traditional fine-tuning.\n\nConclusion\nLoRA is a promising method for fine-tuning LLMs while reducing the computational resources required. It has shown promise for a variety of downstream tasks.\nResources:\nLoRa: Low-Rank Adaptation of Large Language Models\nResource to help understand the paper Explain Paper. It is quite handy would highly recommend\nGoogle Colab Implementation of LoRA"},"ML/Learning-ML/Mamba---Linear-Time-Sequence-Modeling-with-Selective-State-Spaces-Notes":{"title":"Mamba - Linear-Time Sequence Modeling with Selective State Spaces Notes","links":[],"tags":[],"content":""},"ML/Learning-ML/Open-AI-Spinning-Up-Notes":{"title":"Open AI Spinning Up Notes","links":[],"tags":[],"content":"link\nAlgorithms\n\nOn-Policy Algorithms\n\nmost basic, entry level, old\nVanilla Policy Gradient\ncannot reuse old data → weaker on sample efficiency\nalgos optimize for policy performance\nTrade off sample efficiency in favor of stability\nTRPO and PPO\n\n\nOff-Policy Algos\n\nyounger, connected to Q learnging algos, learns Q-funciton and a policy which are updated to improve each other\nCan use old data very efficiently\n\nthey get this benefit through Bellman’s equations for optimality\n\n\nNo guarantee that doing a good job of satisfying bellman’s equations leads to having great policy performance.\n\nEmpirically one can get good performance, and with such a great sample efficiency is wonderful, but the absence of guarantees makes algos in this class brittle and unstable\n\n\nDeep Deterministic Policy Gradients DDPG foundational\nTD3 and SAC are decendents of DDPG\n\n\n\nIntroduction to Reinforced Learning\nPart 1: Key Concepts in RL\nKey Concepts and Terminology\n\nMain characters of RL\n\nAgent - decides what actions to take, and perceives a reward signal from the environment (number saying how good or bad). GOAL maximize cumulative reward, called return.\nEnvironment - world that the agent lives in and interacts with\n\nState s - complete description of the state of the world. There is no hidden information about the world which is not in the state\nObservation o - partial description of a state, which may omit information\nFully Observed- when the agent observes the complete state of the environment\nPartially observed - when the agent sees a partial observation\nAction Spaces - the set of all valid actions in a given environment\n\nDiscrete actions spaces, finite number of moves are available\nContinuous action spaces → normally real-valued vectors\n\ndefinition of stochastic - refers to a process or model that involves some level of randomness or unpredictability.\nPolicy - rule used by an agent to decide what actions to take\n\n\nDeterministic Policy (\\mu) ⇒\n\na_t = \\mu (s_t)\n\na_t is actions to take\n= implies deterministic relationship\n\n\ngiven a particular state, the agent will always take the same action, there is no randomness. Given state s it will always take a\n\n\n\nStochastic Policy (\\pi) -\n\na_t \\sim \\pi (*|s_t)\na_t: This represents the action taken by the agent at time ( t ).\n\\sim: This symbol means “sampled from.” It denotes that the action ( a_t ) is drawn from the probability distribution that follows.\n\nimplies there is a range of actions that could be taken\n\n\n\\pi: This is the policy, which, in the context of a stochastic policy, is a probability distribution over actions.\n| denotes conditional probability. READ as given. Given S you have a\n( \\pi(\\cdot|s_t) ): This is the probability distribution over actions given the current state ( s_t ) at time ( t ). The dot ( \\cdot ) in the parentheses is a placeholder for the action space. It indicates that the policy provides probabilities for all possible actions that can be taken in state ( s_t ).\n\nWhen you see π(a∣s), this is read as “the probability of taking action a given the state s” under the policy π.\n\n\n( s_t ): This is the state of the environment or the agent at time ( t ).\n\n\n\nTypes of Stochastic Policies\n\nCategorical Policies - discrete action space\n\nA categorical policy is like a classifier over discrete actions. You build the neural network for a categorical policy the same way you would for a classifier: the input is the observation, followed by some number of layers (possibly convolutional or densely-connected, depending on the kind of input), and then you have one final linear layer that gives you logits for each action, followed by a softmax to convert the logits into probabilities.\n\n\nDiagonal Gaussian Policies - have a neural network that maps observations to mean actions \\mu_\\theta(s) with added randomness via a vector of standard deviations (diagonal covariance matrix which because it is diagonal all of the variables are independent and all of the “covariance” are 0s) element wise producted with randomness z\n\nThe added randomness with standard deviation can be interpretted as the models uncertainity over it’s choice. This uncertainity can be fixed, or it can be generated by the neural network so that it would have different amounts of uncertainty per input.\n\n\n\n\n\nParameterized Policies - policies whose outputs are computable functions that depend on a set of parameters (weights and biases of a neural network) which we can adjust to change the behavior via some optimization algoritm\n\nParameters of these policies are \\theta or \\phi, then with a subscript to show the connection\n\na_t = \\mu _\\theta (s_t)\na_t \\sim \\pi _\\theta (*|s_t)\n\n\n\n\n\nTrajectories \\tau\n\nA trajectory is a sequence of states and actions\n\n\\tau=\\{s_0,a_0,s_1,a_1,...\\}\ns_0 is randomly sampled from the start-state distribution, s_0 \\sim p_0(\\cdot)\n\n\\cdot acts a placeholder for a variable.\n\n\n\n\n\nState Transitions - what happens to the world between state at time t and the state at t+1 is defined by the laws of the environment (frequently called episodes or rollouts.\n\nDeterministic World Environment: s_{t+1}=f(s_t, a_t)\nStochastic: s_{t+1}\\sim P(\\cdot|s_t,a_t)\n\nWe are sampling the next state (s_{t+1}) from the probability distribution P given the state and the action\nP - stands for state transition probability, it is the probability of ending up in a particular next state\n\n\n\nReward and Return\nReward Function R  takes in the state of the environment, actions, and environment after action.\n\nr_t = R(s_t,a_t,s_{t+1})\nAlso simplified as only dependant on the state of the environment or state-action pari\n\nr_t = R(s_t)\nr_t = R(s_t,a_t)\n\n\nTypes of Return\n\nfinite-horizon undiscounted return - R(\\tau) = \\sum_{t=0}^T r_t\n\nwhich is just a sum of rewards in a fixed window of steps\n\n\ninfinite-horizon discounted return - R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t\n\nwhich is the sum of all rewards ever obtained by the agent, but discounted by how far off in the future they’re obtained. \\gamma \\in (0,1)\nThe goal of the agent is to maximize the notion of cumulative reward over a trajectory (a series of state-action pairs)\n\n\n\n\n\nCentral Problem for RL\nThe central problem for reinforcement learning is maximizing cumulative reward through a optimal policy \\pi^*\nWe can formulate this by \\pi^* = \\arg \\max_{\\pi} J(\\pi). Where J(\\pi) is the expected return across all probabilities of outcomes and their reward. Basically how the reward this policy would have across the total distribution of cases and their reward.\n\nJ(\\pi) = \\int_{\\tau} P(\\tau|\\pi) R(\\tau) = E_{\\tau \\sim \\pi}{R(\\tau)}.\n\nP - stands for state transition probability, it is the probability of ending up in a particular next state\n\n\nP(\\tau|\\pi) = \\rho_0 (s_0) \\prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \\pi(a_t | s_t).\n\nExpected Return J(\\pi)\n\nR(\\tau) is the return of the trajectory \\tau , which is the total accumulated rewqrd from the following trajectory\nthe expected return is J(x) which is the average return that you would expect to get if you were to follow policy \\pi over all trajectories\nIt is calculated as the integral, for each trajectory find the probability of that trajectory occurring P(\\tau|\\pi) times the return of that trajectory R(\\tau)\n\nValue Functions\nValue functions are used when you need to know the value (expected return) of a state or state-action pair,\n\nOn-Policy Value Function V^{\\pi}(s) - gives the expected return if you start in state s and always act according to the policy \\pi\n\nV^{\\pi}(s) = E_{\\tau \\sim \\pi}[R(\\tau)\\left| s_0 = s\\right]\n\nCalculates the expected sum of the rewards an agent would receive given state s and follows policy \\pi for the trajectory.\nThis equation is supposed to predict how good a particular state would be under a policy.\n\n\nE is an expectation operator, and it denotes the expected value of a random variable\n\\tau\\sim\\pi means the trajectories (the list of state and action pairs) sampled according to the policy \\pi\nE_{\\tau \\sim \\pi} means that it is an average over all trajectories starting with s_0\n|s_0=s denotes that it is given starting state s_0 … “Given start state s”\n\n\nOn-Policy Action-Value Function Q^\\pi(s, a) - which gives the expected return if you start with state s, and take an action a (which may have not come from the policy), then forever refer to the policy \\pi for the following acitons\n\nQ^{\\pi}(s,a) = E_{\\tau \\sim \\pi} [R(\\tau)\\left| s_0 = s, a_0 = a\\right]\n\nCalculates the expected sum of the rewards an agent would recieve given a starting action of a and state of s while following policy \\pi for all future actions\n\n\n\n\nOptimal Value Function V^*(s) which gives you the expected return if you start in state s and act with the optimal policy\n\nV^*(s)=max_\\pi [R(\\tau)\\left| s_0 = s\\right]\n\n\nOptimal Action-Value Function Q^*(s,a) - which given a starting state and action, the agent will always act from the optimal policy\n\nQ^*(s,a) = \\max_{\\pi} E_{\\tau \\sim \\pi}[R(\\tau)\\left| s_0 = s, a_0 = a\\right.]\n\n\n\n\nPart 2: Kinds of RL Algorithms\n\nModel of the environment - a function which predicts state transitions and rewards\nIf we have a model we can use it to plan by thinking ahead, seeing what would happen for a range of possible choices. Agents can then distill the results from planning ahead into a learned policy.\nThe main downside of having a model is that a model that perfectly reflects the real world rarely exists. If the agent wants to use a model, it has to learn the model purely from experience"},"ML/Learning-ML/State-Space-Model-(SSM)-and-Structured-State-Space-For-Sequence-Modeling-(S4)":{"title":"State Space Model (SSM) and Structured State Space For Sequence Modeling (S4)","links":["ML/Learning-ML/Mamba---Linear-Time-Sequence-Modeling-with-Selective-State-Spaces-Notes"],"tags":[],"content":"Core Ideas:\n\n\nState Space Models (SSM) in Neural Networks: SSMs are used to model the relationship between input, output, and the internal state of a system. They are particularly effective in handling time-series data and sequences.\n\n\nFrom Continuous to Discrete: SSMs are adapted from continuous-time models (used for analog signals) to discrete-time models (for digital signals) using methods like the bilinear transform, making them suitable for processing discrete input sequences.\n\n\nRecurrent Representation and Efficiency: In their basic form, SSMs have a recurrent nature, which isn’t efficient for modern parallel processing hardware. This is overcome by transforming them into a discrete convolution form.\n\n\nConvolutional Approach: By representing SSMs as discrete convolutions, they can be computed more efficiently on modern hardware like GPUs, enabling faster processing of large datasets.\n\n\nSSM Neural Network Implementation: These concepts are implemented in neural networks using specialized layers and transformations, with applications in processing sequences, such as in time-series analysis or natural language processing.\n\n\nContext and Relevance\n\n\nAdvances in Sequence Modeling: Traditional models like RNNs (Recurrent Neural Networks) face challenges in handling long sequences due to their sequential nature and limitations in memory. SSMs offer a more efficient and scalable alternative.\n\n\nEfficiency and Parallelism: The transformation of SSMs into a form suitable for parallel processing aligns well with the capabilities of modern GPUs, leading to significant improvements in computational efficiency.\n\n\nApplication in Deep Learning: SSMs find applications in various domains within deep learning, such as language modeling, signal processing, and even in complex tasks like forecasting and anomaly detection in time-series data.\n\n\nTheoretical Innovation: The integration of concepts like HiPPO matrices into SSMs represents a blend of theoretical innovation with practical application, enabling models to effectively capture and remember long-range dependencies in data.\n\n\nFuture of Machine Learning: The exploration and implementation of SSMs in neural network architectures contribute to the ongoing evolution of machine learning, particularly in making models more efficient, scalable, and capable of handling complex, sequential data.\n\n\nRecent interesting paper to show how innovative this design really is\nNotes on the Annotated S4\nGoal is the efficient modeling of long sequences.\nWe are going to build a new neural network layer based on State Space Models.\nState Space Model (SSM) - 1-D input signal u(t) to a N-D latent state x(t) before projecting to a 1-D output signal y(t)\n\nu(t) - input signal 1-D\n\nu of t part emphasizes that the input is over time\n\n\ny(t) - output signal 1-D\nx(t) - latent state N-D\n\nmulti-dimensional internal state of the State Space Model\nN-dimensional\n\n\n\nx&#039;(t)=Ax(t)+Bu(t)\ny(t) = Cx(t)+Du(t)\n\nWith D = 0, the main Idea is that we use the input and run it through B, which then we take the internal state x(t) and use A to generate the new state x’(t). With that new state we run it through C which then is the output y(t).\n\n\nA, B, C, D parameters learned over gradient descent\nD = 0, it is a skip connection; rather skip over Du(t)\n\nDefining A, B, C\n# initalize parameters A, B, C\ndef random_SSM(rng, N):\n    a_r, b_r, c_r = jax.random.split(rng, 3)\n    A = jax.random.uniform(a_r, (N, N))\n    B = jax.random.uniform(b_r, (N, 1))\n    C = jax.random.uniform(c_r, (1, N))\n    return A, B, C\nDiscrete-Time SSM: The Recurrent Representation\nIdea: instead of the input being a continuous function u(t) we make the input (u_0, u_1, ...). The input is discretized by a step size Δ\nWe cannot take A, B, C and leave them the way they are. We have to use a bilinear method to convert the state matrix A in to an approximation \\bar{A}  Ref\n\n\\bar{A}=(I-Δ/2⋅2)^{-1}(I+Δ/2⋅A)\n\\bar{B}=(I-Δ/2⋅A)^{-1}ΔB\n\\bar C = C\n\n# Take randomly generated parameters A, B, C and use a bilinear transformation\n# so that the parameters work with a discrete time control model\ndef discretize(A, B, C, step):\n    I = np.eye(A.shape[0])\n    BL = inv(I - (step / 2.0) * A)\n    Ab = BL @ (I + (step / 2.0) * A)\n    Bb = (BL * step) @ B\n    return Ab, Bb, C\nOnce we discretize the model, it can be viewed and calculated like an RNN\n\nx_{k}= \\bar A x_{k-1} + \\bar B u_{k}\ny_{k}= \\bar C x_k\n\ndef scan_SSM(Ab, Bb, Cb, u, x0):\n    def step(x_k_1, u_k):\n        x_k = Ab @ x_k_1 + Bb @ u_k\n        y_k = Cb @ x_k\n        return x_k, y_k\n \n    return jax.lax.scan(step, x0, u)\nPutting everything together to run the SSM\ndef run_SSM(A, B, C, u):\n    L = u.shape[0]\n    N = A.shape[0]\n    Ab, Bb, Cb = discretize(A, B, C, step=1.0 / L)\n \n    # Run recurrence\n    return scan_SSM(Ab, Bb, Cb, u[:, np.newaxis], np.zeros((N,)))[1]\nCore idea: Recurrent Neural Networks are slow because we cannot do multiple calculations in parallel, we have to do them sequentially. One after another. Because Convolutional Neural Networks are optimized on hardware, people try to turn an RNN into a CNN. They do this by “Unrolling” the RNN into a CNN. They do this by converting the RNN into a discrete convolution.\n\nDiscrete Convolution is like blending two signals or inputs together to create a new sequence. The idea is to combine them while still retaining some of the values meaning. so like bit shifting the values then encoding the values in the open gap.\n\nWe this concept implemented we can use parallelism of convolution operations\n\n\n\nHere are the new equations:\ny_{k}= \\bar C \\bar A^{k} \\bar Bu_{0} + \\bar C \\bar A^{k-1} \\bar Bu_{1}+ ... + \\bar C \\bar A \\bar Bu_{k-1} + \\bar C \\bar Bu_k\ny = \\bar K * u where \\bar K is the SSM convolutional Kernel\n\\bar K ∈ R^L=(\\bar{CB},\\bar{CAB}, ..., \\bar{CA^{L-1}B})\n&lt;Warning this code is bad and won’t work for small lengths&gt;\n“Note that this is a giant filter. It is the size of the entire sequence!”\n# This function computes the K kernal shown above\n# it should be an array of the computed parameters A, B, C\ndef K_conv(Ab, Bb, Cb, L):\n    return np.array(\n        [(Cb @ matrix_power(Ab, l) @ Bb).reshape() for l in range(L)]\n    )\nHere is the math to get those equations\nReference equations\nx_{k}= \\bar Ax_{k-1} + \\bar Bu_k\ny_{k}= \\bar C x_k\nx_{-1}=0\nSince the main equation is y=\\bar K *u, we still need to multiply them together. We can use Fast Fourier Transform (FFT) or Convolution theorem, to speed this up. To use this theorem we need to pad the input sequences with zeros then unpad the output sequence. As the length gets longer this FFT method will be more efficient than direct convolution.\n# fast function for K * u\n# this uses Fourier Transform (FFT) or Convolution theorem\ndef causal_convolution(u, K, nofft=False):\n    if nofft:\n        return convolve(u, K, mode=&quot;full&quot;)[: u.shape[0]]\n    else:\n        assert K.shape[0] == u.shape[0]\n        ud = np.fft.rfft(np.pad(u, (0, K.shape[0])))\n        Kd = np.fft.rfft(np.pad(K, (0, u.shape[0])))\n        out = ud * Kd\n        return np.fft.irfft(out)[: u.shape[0]]\n# testing function \ndef test_cnn_is_rnn(N=4, L=16, step=1.0 / 16):\n    ssm = random_SSM(rng, N)\n    u = jax.random.uniform(rng, (L,))\n    jax.random.split(rng, 3)\n    # RNN\n    rec = run_SSM(*ssm, u)\n \n    # CNN\n    ssmb = discretize(*ssm, step=step)\n    conv = causal_convolution(u, K_conv(*ssmb, L))\n \n    # Check\n    assert np.allclose(rec.ravel(), conv.ravel())\nSSM Neural Network\nDiscrete SSM defines a map from R^{L}-&gt; R^{L} a 1-D sequence map.\nWe assume that we are going to be learning parameters B and C, as well as step size \\Delta and a scalar D. For parameter A we will be using a HiPPO matrix. We learn the step size in log space.\ndef log_step_initializer(dt_min=0.001, dt_max=0.1):\n    def init(key, shape):\n        return jax.random.uniform(key, shape) * (\n            np.log(dt_max) - np.log(dt_min)\n        ) + np.log(dt_min)\n \n    return init\nMost of the SSM layer work is building the kernel (filter).\nthe self.decode specifies if the SSMLayer is in CNN mode or RNN mode\nclass SSMLayer(nn.Module):\n    N: int\n    l_max: int\n    decode: bool = False\n \n    def setup(self):\n        # SSM parameters\n        self.A = self.param(&quot;A&quot;, lecun_normal(), (self.N, self.N))\n        self.B = self.param(&quot;B&quot;, lecun_normal(), (self.N, 1))\n        self.C = self.param(&quot;C&quot;, lecun_normal(), (1, self.N))\n        self.D = self.param(&quot;D&quot;, nn.initializers.ones, (1,))\n \n        # Step parameter\n        self.log_step = self.param(&quot;log_step&quot;, log_step_initializer(), (1,))\n \n        step = np.exp(self.log_step)\n        self.ssm = discretize(self.A, self.B, self.C, step=step)\n        self.K = K_conv(*self.ssm, self.l_max)\n \n        # RNN cache for long sequences\n        self.x_k_1 = self.variable(&quot;cache&quot;, &quot;cache_x_k&quot;, np.zeros, (self.N,))\n \n    def __call__(self, u):\n        if not self.decode:\n            # CNN Mode\n            return causal_convolution(u, self.K) + self.D * u\n        else:\n            # RNN Mode\n            x_k, y_s = scan_SSM(*self.ssm, u[:, np.newaxis], self.x_k_1.value)\n            if self.is_mutable_collection(&quot;cache&quot;):\n                self.x_k_1.value = x_k\n            return y_s.reshape(-1).real + self.D * u\nSSM operate on scalars, we make H different, stacked copies\ndef cloneLayer(layer):\n    return nn.vmap(\n        layer,\n        in_axes=1,\n        out_axes=1,\n        variable_axes={&quot;params&quot;: 1, &quot;cache&quot;: 1, &quot;prime&quot;: 1},\n        split_rngs={&quot;params&quot;: True},\n    )\nSSMLayer = cloneLayer(SSMLayer)\nSSM Layer can then be put into a standard NN. We also add a block that pairs a call to an SSM with dropout and linear projection\nclass SequenceBlock(nn.Module):\n    layer_cls: nn.Module\n    layer: dict  # Hyperparameters of inner layer\n    dropout: float\n    d_model: int\n    prenorm: bool = True\n    glu: bool = True\n    training: bool = True\n    decode: bool = False\n \n    def setup(self):\n        self.seq = self.layer_cls(**self.layer, decode=self.decode)\n        self.norm = nn.LayerNorm()\n        self.out = nn.Dense(self.d_model)\n        if self.glu:\n            self.out2 = nn.Dense(self.d_model)\n        self.drop = nn.Dropout(\n            self.dropout,\n            broadcast_dims=[0],\n            deterministic=not self.training,\n        )\n \n    def __call__(self, x):\n        skip = x\n        if self.prenorm:\n            x = self.norm(x)\n        x = self.seq(x)\n        x = self.drop(nn.gelu(x))\n        if self.glu:\n            x = self.out(x) * jax.nn.sigmoid(self.out2(x))\n        else:\n            x = self.out(x)\n        x = skip + self.drop(x)\n        if not self.prenorm:\n            x = self.norm(x)\n        return x\nWe then stack a bunch of the blocks on top of each other to produce a stack of SSM layers.\nclass Embedding(nn.Embed):\n    num_embeddings: int\n    features: int\n \n    @nn.compact\n    def __call__(self, x):\n        y = nn.Embed(self.num_embeddings, self.features)(x[..., 0])\n        return np.where(x &gt; 0, y, 0.0)\nclass StackedModel(nn.Module):\n    layer_cls: nn.Module\n    layer: dict  # Extra arguments to pass into layer constructor\n    d_output: int\n    d_model: int\n    n_layers: int\n    prenorm: bool = True\n    dropout: float = 0.0\n    embedding: bool = False  # Use nn.Embed instead of nn.Dense encoder\n    classification: bool = False\n    training: bool = True\n    decode: bool = False  # Probably should be moved into layer_args\n \n    def setup(self):\n        if self.embedding:\n            self.encoder = Embedding(self.d_output, self.d_model)\n        else:\n            self.encoder = nn.Dense(self.d_model)\n        self.decoder = nn.Dense(self.d_output)\n        self.layers = [\n            SequenceBlock(\n                layer_cls=self.layer_cls,\n                layer=self.layer,\n                prenorm=self.prenorm,\n                d_model=self.d_model,\n                dropout=self.dropout,\n                training=self.training,\n                decode=self.decode,\n            )\n            for _ in range(self.n_layers)\n        ]\n \n    def __call__(self, x):\n        if not self.classification:\n            if not self.embedding:\n                x = x / 255.0  # Normalize\n            if not self.decode:\n                x = np.pad(x[:-1], [(1, 0), (0, 0)])\n        x = self.encoder(x)\n        for layer in self.layers:\n            x = layer(x)\n        if self.classification:\n            x = np.mean(x, axis=0)\n        x = self.decoder(x)\n        return nn.log_softmax(x, axis=-1)\nBatchStackedModel = nn.vmap(\n    StackedModel,\n    in_axes=0,\n    out_axes=0,\n    variable_axes={&quot;params&quot;: None, &quot;dropout&quot;: None, &quot;cache&quot;: 0, &quot;prime&quot;: None},\n    split_rngs={&quot;params&quot;: False, &quot;dropout&quot;: True},\n)\nThe full code is listed here\nProblems with SSMs\n\nRandomly initialized SSM does not perform well\nComputing it naively like we’ve done so far is really slow and memory inefficient\n\nPart 1b: Addressing Long-Range Dependencies with HiPPO\nPrior Work found that SSMs dont work in practice because gradients scaling exponentially in the sequence length. HiPPO theory comes in to help.\nThe idea is to use the HiPPO Matrix, which tries to memorize the history of the input. they define the most important matrix as a HiPPO matrix. The HiPPO Matrix is kind of complicated, look up for yourself, doesn’t seem to be very relevant to the understanding of S4.\nbenefits of making A an HiPPO Matrix:\n\nA only needs to be calculated once.\nMatrix aims to compress the past history into a state that has enough information to reconstruct the history.\n\nPrior work found that it was very successful moving A from random to a HiPPO matrix.\ndef make_HiPPO(N):\n    P = np.sqrt(1 + 2 * np.arange(N))\n    A = P[:, np.newaxis] * P[np.newaxis, :]\n    A = np.tril(A) - np.diag(np.arange(N))\n    return -A\nDiving deeper into HiPPO matrices\nThey are successful through coefficients of a Legendre polynomials. These coefficients let it approximate all of the previous history.\ndef example_legendre(N=8):\n    # Random hidden state as coefficients\n    import numpy as np\n    import numpy.polynomial.legendre\n \n    x = (np.random.rand(N) - 0.5) * 2\n    t = np.linspace(-1, 1, 100)\n    f = numpy.polynomial.legendre.Legendre(x)(t)\n \n    # Plot\n    import matplotlib.pyplot as plt\n    import seaborn\n \n    seaborn.set_context(&quot;talk&quot;)\n    fig = plt.figure(figsize=(20, 10))\n    ax = fig.gca(projection=&quot;3d&quot;)\n    ax.plot(\n        np.linspace(-25, (N - 1) * 100 + 25, 100),\n        [0] * 100,\n        zs=-1,\n        zdir=&quot;x&quot;,\n        color=&quot;black&quot;,\n    )\n    ax.plot(t, f, zs=N * 100, zdir=&quot;y&quot;, c=&quot;r&quot;)\n    for i in range(N):\n        coef = [0] * N\n        coef[N - i - 1] = 1\n        ax.set_zlim(-4, 4)\n        ax.set_yticks([])\n        ax.set_zticks([])\n        # Plot basis function.\n        f = numpy.polynomial.legendre.Legendre(coef)(t)\n        ax.bar(\n            [100 * i],\n            [x[i]],\n            zs=-1,\n            zdir=&quot;x&quot;,\n            label=&quot;x%d&quot; % i,\n            color=&quot;brown&quot;,\n            fill=False,\n            width=50,\n        )\n        ax.plot(t, f, zs=100 * i, zdir=&quot;y&quot;, c=&quot;b&quot;, alpha=0.5)\n    ax.view_init(elev=40.0, azim=-45)\n    fig.savefig(&quot;images/leg.png&quot;)\nif False:\n    example_legendre()\nEach is a coefficient for one element of the Legendre series shown as blue functions. The intuition is that the HiPPO matrix updates these coefficients each step.\nS4 in Practice\nReally cool experiments using the S4 in Practice.\nConclusions\nMy goal for this was to learn more about SSM and S4. Going Through the Annotated S4 was to understand enough about the foundations to go through Mamba - Linear-Time Sequence Modeling with Selective State Spaces Notes, which has been all of the hype lately.\nInnately the idea of a state plus all of the cognitive abilities that we are now seeing makes rational sense. Transformers only have so much “memory” in their context window. Each increase in the models context windows leads to a quadratic increase in size, this is not sustainable or smart. Future innovations will blend state and attention to create a more sensible structure."},"Projects/How-My-Personal-Website-Works":{"title":"How My Personal Website Works","links":[],"tags":[],"content":"What is my personal website\nThe website exists here at ethanmorgan.io. Here is all of the code stored on a github repo here\nThere are 2 main views:\n\nhomepage ethanmorgan.io\n\nGoal to be friendly for both technical and non-technical audiences\n\nI have told senior engineers about my website before, so I want it to look good for them and →\nI have showed it to my grandma, and I want it to look cool for her rather than it being a bunch of text on a blank page.\n\n\n\n\nblog ethanmorgan.io/blog\n\nGoal to have a nice place to write and show off my work\n\n\n\nEach are made from different templates using difference resources. I will go into how each are made below.\nHow does my personal website work?\nHomepage\nI created the homepage from the template from StyleShout Luther design, which I think is a fantastic resource. I have made a couple of modifications to the original template such as:\n\nmaking the website fade in faster rather than taking a long time\nMaking the circles on the right hand side appear bolder\nchange the color template to shades of orange\nchange the font to “Be Vietnam Pro” which I like very much\nadd a blog\n\nBlog\nI created the blog from  the template generator Quartz and I am using Obsidian to write my notes. I have a small script that I used to rebuild my blog each time I update it. updateBlogScript.sh\nI use this script to regenerate all of the static files then move them into my website repo.\nHosting setup\nRight now the domain name is registered under AWS Route53 and I have an AWS Amplify free tier website setup that will receives changes from my github repo. This github repo contains all of the html and content needed to serve the static website.\nWhy is it setup the way it is?\nThis design has been in constant iteration for a long time. Most of the changes come from my preferences. I am not great with css and html, so designing a personal website from scratch is a lot of work and time. Although it may be worth while to make and learn, practically I can just take a template I like and reformat it to be what I want.\nWhy not use github pages?\nGithub pages are a great solution, when I started making my website they did have a lot of compelling features that make it simple for users. But I wanted more control over my website where I could add some more functional features that Github could not provide. Plus why would I take the easier solution when I can reinvent the wheel for the almost same solution\n\nHow My Website Works References"},"index":{"title":"Welcome To My Blog!","links":[],"tags":[],"content":"Hi Welcome to my Blog!\nI am Ethan Morgan and here are some of my notes!\nI currently work at Amazon in their AGI lab in San Francisco. I am currently working under the recently acquired org from Adept AI (here).\nMany of the notes that I currently take are for internal Amazon and it is hard to pull them into the public domain. When I get the chance I will write here."}}