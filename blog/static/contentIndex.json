{"Amazon-Layoffs":{"title":"The 2023 AWS Layoffs and Personal Insights","links":[],"tags":["Layoffs","Amazon","AWS"],"content":"In an unprecedented turn of events, Amazon Web Services (AWS) recently faced a round of layoffs. This is a first in the history of AWS, marking a break from the past when even the tumultuous times of the 2008 financial crisis saw no layoffs at Amazon. I would like to share my insights and experiences related to this significant event.\nThe Announcement\nMy journey as a full-time software engineer at AWS was barely a month old, fresh from my university graduation, when the startling news of layoffs came through. It wasn’t an entirely new phenomenon at Amazon, as several divisions had previously experienced job cuts, mostly because they were not profitable. Hence, a reduction in expenditure was a strategic move towards achieving financial stability. But the AWS scenario was a different ball game. It was a profit hub, contributing approximately 50% of Amazon’s overall earnings. Consequently, layoffs within AWS seemed unwarranted, at least in my initial perspective. But this notion was dispelled when I read from TechCrunch: “Amazon confirms another round of layoffs, impacting 9,000 people in AWS, Twitch and other units.” (March 20, 2023).\nThis announcement sparked an influx of doubts and questions concerning the criteria for the layoffs. Despite reassurances that our team’s indispensable product would shield us from the layoffs, my anxiety levels were still heightened, primarily because I was the newest team member. The prospect of being the first exit due to my recent entry was quite nerve-wracking. But I was not the only one feeling the pressure. A surge in the internal Slack channel’s ‘layoff-discussion,’ with its membership skyrocketing to 60,000. The channel was abuzz with discussions about the impending layoffs, their timing, location, and the general emotional roller-coaster ride that ensued.\nThe Layoff\nThe dreaded day, Wednesday, April 26th, 2023, dawned with approximately 8,000 AWS employees receiving their termination emails at precisely 7:00 AM. Barely three minutes later, access to all company resources, including Slack, development environments, and emails, was revoked for those affected. My phone buzzed at 7:05 AM with a heartbreaking message from a close friend and talented engineer who was among the unfortunate ones. My organization had 27 people across 460 leave. 1 was in my team and 2 from our sister team.\nWhile I was about four levels below the decision-making executives, I heard the layoffs were generally orchestrated at the director level. However, the experiences varied across different departments. My analysis of the AWS Cryptography division, where I work, revealed a trend. The layoffs targeted underperformers, employees in non-aligned locations, non-software engineer roles, and SDE 1s (the entry level software engineer role). Non-aligned locations referred to remote employees or those working away from their teams or managers. Two of the best engineers I knew were laid off due to location mismatches.\nThe possibility of layoffs in our team was quite bewildering, given our product’s vital role as a Tier 0 product crucial to all internal services. Surprisingly, even teams associated with the most profitable AWS services like EC2, Lambda, and S3 experienced layoffs. It appeared as if stringent criteria were set, and anyone failing to meet these criteria was shown the door."},"First-Blog":{"title":"My Hello World","links":[],"tags":[],"content":"Does it ever start without a Hello World?"},"I-felt-the-World-Change;-The-Release-of-ChatGPT":{"title":"I felt the World Change: The Release of ChatGPT","links":[],"tags":["GPT"],"content":"On December 1st, 2022, I watch the world change. As I sat in front of my computer, I couldn’t believe my eyes. As I continued to use chatGPT, I was struck by how intuitive and user-friendly it was. I didn’t need to have any prior knowledge or experience with language models to be able to use it effectively. Its user interface was simple and straightforward, making it easy for anyone to start using chatGPT right away.\nOne of the most impressive aspects of chatGPT was its ability to understand and respond to natural language. I could ask it questions in the same way that I would ask a friend, and it would provide detailed and accurate answers. This made it incredibly easy to have a conversation with chatGPT, and I found myself using it more and more as a go-to source for information.\nI was also impressed by the speed at which chatGPT was able to generate text. I could ask it to generate a paragraph or even an entire essay in a matter of seconds, and the resulting text was always of high quality. This made it an incredibly useful tool for anyone who needs to produce written content quickly and efficiently.\nOverall, my experience with chatGPT was nothing short of amazing. It was an incredible tool that has the potential to revolutionize the way we access and use information. I can’t wait to see what the future holds for chatGPT and the world of artificial intelligence.\nIn fact ChatGPT wrote this entire blog post.\nUpdate: Jan. 05, 2023\nHi, the human here.\nYes that was a little ingenuine on my part for leading you on like that, I am sorry about that. But it does a great job right?\nBut I really want to talk about this incredible new and exciting tool. For a long time I always had wondered what it would be like to experience a new and life changing product. The biggest game changing product that I remember in my life was the iphone. I was a child then and I vaguely remember it. Even then I could not conceptualize what this product would really was. And for a long time I wished that I could experience this change and to see how life would change forever because of it.\nHere is a quick list of how long it took modern tech websites to reach 1 million active daily users:\nAirbnb: 30 months\nTwitter: 24 months\nFacebook: 10 months\nDropbox: 7 months\nSpotify: 5 months\nInstagram: 2.5 months\nChatGPT: 5 days\nThe craziest part is all of my interactions about this product were ONLY communicated by talking to other people. No social media campaigns. No podcast shoutouts. No finding on twitter. Just from friends and seeing people use it. 5 days"},"Learnings/temp":{"title":"temp","links":[],"tags":[],"content":""},"LoRa-Low-Rank-Adaptation-of-Large-Language-Models":{"title":"LoRa: Low Rank Adaptation of Large Language Models Paper Review","links":[],"tags":["LLMs","Machine-Learning"],"content":"LoRa: Low Rank Adaptation of Large Language Models Paper Review\nHere is the paper I will be writing about today. LoRa: Low-Rank Adaptation of Large Language Models Paper\n1. Overview/Background:\nIntroduction to Large Language Models and Training:\nLarge Language Models (LLMs), consisting of massive matrices filled with numbers running into hundreds of gigabytes, are sophisticated tools in the AI sphere. Training these models typically involves two steps - pre-training and fine-tuning.\nPre-training: This phase involves training the model on a vast corpus of text, enabling it to acquire general language capabilities. During this stage, the model primarily serves as a ‘next-word predictor.’ The learning process during this phase is computationally expensive, thus incurring most of the operation cost. The model assimilates a wealth of knowledge but lacks the capability to effectively output it. For example, if the model is posed with a question, it might respond with related questions rather than providing a succinct answer.\nFine-tuning: This phase ensues post pre-training, where the model is trained on a smaller, task-specific dataset to acquire desired responses. For example, if the model is meant to function like chatGPT, it would be trained on a dataset of Questions and Answers similar to chatGPT. The training in this phase features smaller ‘steps,’ ensuring that each iteration only slightly alters the model’s weights.\n2. The Problem:\nThe significant issue with full fine-tuning is that every training iteration changes all the model parameters. This implies that the model needs to relearn and adjust all parameters for each iteration, which for larger models like GPT3 (with 175 Billion parameters) is a colossal computational task. Furthermore, fine-tuning the model for each task multiplies the storage needs. These challenges make the deployment of multiple instances of fine-tuned models practically infeasible.\n3. Solution\nLoRa addresses this challenge by proposing a low-rank adaptation of the model.\nGeneralized Formula\n\nOutput vector: Weights * Input vector\nWeights of pretrained LLM: W_0\nFormula for calculating weight matrices when finetuning: W_1 = W_0 + \\Delta W\nFormula for using finetuned weight matrices: h = W_1 * x = W_0 * x + \\Delta W * x\n\nWhat LoRA Proposes\nLoRA proposes that we do rank decomposition on the change W into two smaller matrices A and B. A and B will try to capture the change in W into smaller matrices that are easier to train and deploy. The formula for rank decomposition is as follows:\n\\Delta W = A * B\nwhere A and B are the rank-decomposition matrices.\nThen substite in the rank-decomposition matrices into the finetuning equation: h = W_0 * x + A * B * x\nDuring the finetuning process, the pre-trained weights are frozen, with modifications permitted only in A and B. Setting a low rank can substantially reduce the parameters to tune.\nBenefits of LoRA\n\nReduced training time: LoRA can significantly reduce the training time of LLMs, since only a small number of parameters need to be updated.\nReduced memory requirements: LoRA can also reduce the memory requirements of LLMs, since the number of parameters is much smaller.\nLess prone to catastrophic forgetting: LoRA is less prone to catastrophic forgetting, since the pre-trained weights are not updated.\nEasier to deploy: LoRA weights are easier to deploy than full fine-tuned models, since they are much smaller.\n\nLimitations of LoRA\n\nMay not be as effective as full fine-tuning: LoRA may not be as effective as full fine-tuning for some tasks.\nMay require more hyperparameter tuning: LoRA may require more hyperparameter tuning than traditional fine-tuning.\n\nConclusion\nLoRA is a promising method for fine-tuning LLMs while reducing the computational resources required. It has shown promise for a variety of downstream tasks.\nResources:\nLoRa: Low-Rank Adaptation of Large Language Models\nResource to help understand the paper Explain Paper. It is quite handy would highly recommend\nGoogle Colab Implementation of LoRA"},"My-Journey;-High-School-to-Amazon":{"title":"My Journey: High School to Amazon","links":[],"tags":["Amazon","Learnings"],"content":""},"index":{"title":"Welcome To My Blog!","links":[],"tags":[],"content":"Hi Welcome to my Blog!\nI am Ethan Morgan and here are some of my notes!"}}